---
title: "Technical Report: Building Price Prediction Models For Copenhagen Apartments"
author: "Shahana Ayobi"
date: '2023-02-07'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE, message=FALSE, warning=FALSE}
# Loading the packages and the data
rm(list=ls())
# Loading the Libraries
library(tidyverse)
library(caret)
library(modelsummary)
library(stargazer)
library(xtable)
library(rattle)
library(kableExtra)
library(data.table)
library(ggplot2)
library(GGally)
library(gridExtra)
library(knitr)
library(viridis)
library(directlabels)
library(Hmisc)
library(cowplot)
library(ranger)
library(glmnet)
library(grid)
library(skimr)
library(gbm)
library(fixest)
library(rpart)
library(rpart.plot)

path <- "/Users/shahanaayobi/Desktop/RWork/Data-Analysis-3/Assignment2/"

#location folders
data_in  <- paste0(path,"/data/raw/")
data_out <- paste0(path,"/data/clean/")
output <- paste0(path, "/output/")
# Reading the data
df <- read.csv(paste0(data_in,"listings.csv"), fileEncoding="UTF-8")

```

# Introduction
This report's objective is to give a thorough explanation of the price prediction model. The primary objective of this project is to assist a business in setting a price for brand-new flats that have not yet hit the market. The data is gathered from Inside Airbnb, which can be obtained [here](http://insideairbnb.com/get-the-data), to develop a price prediction model for a business operating small and mid-size apartments hosting two to six guests in Copenhagen, Denmark. Five price prediction models—OLS, Lasso, Cart, Random Forest, and GBM, Gradient Boosting Machine—are produced as a result of data cleaning, munging, and analysis. GBM therefore displayed the best prediction outcome with a 65.38 USD RMSE. The most important predictor features are the neighborhood, reviews per month, review scores rating, number of accommodates, property type, the number of bathrooms, and the number of beds. Other factors, such as the number of days since the first review, are also significant predictors. The project's ultimate goal is to complete a better prediction model as measured by relative RMSE values.

# Data Cleaning 
The original data set consists of a single data table with 75 columns and 13,820 observations. The following are some of the crucial columns: ID, price, reviews per month, number of accommodates, property type, amenities, and other features specific to the host and rental property. The information relates to one-night rental rates for the period of December 29, 2022. Price per night, per person, expressed in Danish Krone, is the desired variable which is then converted to US dollars by by multiplying it to market exchange rate of 0.14.
Because working with Tidy data tables is easier, the original data set needs to be cleaned because it contains a lot of information. At first, I removed columns for the host's website, the host's profile photo, the house rules, notes, and other information. Because this project does not include these columns as a target. Additionally, transforming the "amenities" column into binary variables makes up a large portion of the transformation procedure used to convert the original data into a tidy data table. Only dummy variables for amenities that were important for the analysis have been created such as having TV, coffee maker, wifi, heating, microwave and so on. More information on the data cleaning can be found on the code.
Additionally, following the initial stage of cleaning, follows data preparation. The following phases make up the process of preparing data for analysis: determining the types of the variables, getting rid of duplicates, and dealing with missing values. The chosen data collection, Airbnb Copenhagen, includes binary variables like all amenities and host attributes along with numeric values like price and factor variables like neighborhood and property type. Thus, I generated factor variables for predictors like neighborhood and property types.

**Filters** The data was filtered in accordance with the project's primary objective of predicting apartment prices for units that fall between 2 and 6 accommodates. Price per night included extreme values exceeding 1000 USD per night, which comprised fewer than 1% of the observations, therefore price was filtered to less than 600 USD and the observation where price is missing was dropped. Additionally, it is essential to examine prices and the log of price distribution because the project's objective is to create a price prediction model. As shown below the price distribution is skewed with a long right tail while log price distribution is close to normal. However, prediction is carried out on price per night for model simplicity.


```{r include=FALSE, message=FALSE, warning=FALSE}
# Dropping uneccessary columns
drops <- c("listing_url", 
           "scrape_id", 
           "name",
           "description",
           "host_thumbnail_url",
           "neighborhood_overview",
           "host_picture_url",
           "thumbnail_url",
           "medium_url",
           "picture_url",
           "neighbourhood_group_cleansed" ,
           "license",
           "host_name",
           "xl_picture_url",
           "host_url",
           "experiences_offered", 
           "notes", 
           "transit", 
           "access", 
           "interaction", 
           "house_rules", 
           "host_about", 
           "summary", 
           "space", 
           "host_location",
           "host_total_listings_count",
           "source",
           "bathrooms",
           "minimum_minimum_nights","maximum_maximum_nights","minimum_maximum_nights",
           "maximum_minimum_nights","minimum_nights_avg_ntm","maximum_nights_avg_ntm", 
           "number_of_reviews_ltm", "is_business_travel_ready", 
           "calculated_host_listings_count_entire_homes", 
           "calculated_host_listings_count_private_rooms", 
           "calculated_host_listings_count_shared_rooms",
           "calendar_updated", "review_scores_accuracy",
           "host_since", "review_scores_checkin", "review_scores_location",
           "review_scores_value", "review_scores_communication"
)

df <- df[ , !(names(df) %in% drops)]
# Checking which variables have a lot of missing values
colSums(is.na(df))

# A lot of NAs for the following variables that may be not that significant:
# "host_neighbourhood" 8473
# "neighbourhood" 6946

# Dropping them 
drops <- c("host_neighbourhood", "neighbourhood", "bedrooms", "last_review",
           "host_response_time",
           "review_scores_cleanliness", "host_response_rate", "last_scraped")
df <- df[ , !(names(df) %in% drops)]

# removing dollar signs and comma from price variable
  for (i in 1:nrow(df)){
    df$price[i] <- gsub("\\$","",as.character(df$price[i]))
    df$price[i] <- as.numeric(gsub(",", "", as.character(df$price[i])))
  }
# remove percentage signs 
for (per in c("host_acceptance_rate")){
  df[[per]]<- as.numeric(gsub("%","",as.character(df[[per]])))
}


# assigning values to string binary variables that have TRUE and FALSE values and changing them to 1 and 0 successively
for (binary in c("host_is_superhost","host_has_profile_pic","host_identity_verified", "has_availability", "instant_bookable")){
  df[[binary]][df[[binary]]=="f"] <- 0
  df[[binary]][df[[binary]]=="t"] <- 1
}



```


```{r message=FALSE, warning=FALSE, include=FALSE}
# filtering for number of guests 
df <- df %>%  filter(df$accommodates >= 2 & df$accommodates <= 6)


# selecting the property types: keeping only apartments
df <- df %>% filter(property_type %in% c("Entire loft", "Entire condo",
                                         "Entire serviced apartment", "Entire home/apt","Entire rental unit"))


df <- df %>%  mutate(p_host_acceptance_rate = as.numeric(host_acceptance_rate))

# get the number of baths from bathroom_text
df <- df %>% rename(bathrooms = bathrooms_text)
df$bathrooms <- as.numeric(gsub("[^0-9.-]", "", gsub("half", 0.5, df$bathrooms, ignore.case = T)))
unique(df$bathrooms)

#create days since first review
df <- df %>%
  mutate(
    n_days_since = as.numeric(as.Date(calendar_last_scraped,format="%Y-%m-%d") -
                                as.Date(first_review ,format="%Y-%m-%d")))
# Change Infinite values with NaNs
for (j in 1:ncol(df)) data.table::set(df, which(is.infinite(df[[j]])), j, NA)

# where do we have missing variables now?
to_filter <- sapply(df, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

# imputing when few, not that important
df <- df %>%
  mutate(
    bathrooms =  ifelse(is.na(bathrooms), median(bathrooms, na.rm = T), bathrooms), #assume at least 1 bath
    beds = ifelse(is.na(beds), accommodates, beds), #assume beds=accommodates
    reviews_per_month=ifelse(is.na(reviews_per_month ),1, reviews_per_month), 
  )


# Replacing missing variables reviews with zero, when no review + add flags

df <- df %>%
  mutate(
    flag_days_since=ifelse(is.na(n_days_since),1, 0),
    n_days_since =  ifelse(is.na(n_days_since), median(n_days_since, na.rm = T), n_days_since), 
    flag_reviews_per_month=ifelse(is.na(reviews_per_month),1, 0),
    reviews_per_month =  ifelse(is.na(reviews_per_month), median(reviews_per_month, na.rm = T), reviews_per_month),
    flag_number_of_reviews=ifelse(number_of_reviews==0,1, 0),  flag_review_scores_rating=ifelse(is.na(review_scores_rating),1, 0),  review_scores_rating =  ifelse(is.na(review_scores_rating), median(review_scores_rating, na.rm = T), review_scores_rating)
 )

# Renaming the property types
df$property_type <- df$property_type %>% replace(df$property_type == 'Entire home/apt', "Apartment")
df$property_type <- df$property_type %>% replace(df$property_type == 'Entire serviced apartment', "Apartment")
df$property_type <- df$property_type %>% replace(df$property_type == 'Entire condo', "Condominium")
df$property_type <- df$property_type %>% replace(df$property_type == 'Entire loft', "Loft")
df$property_type <- df$property_type %>% replace(df$property_type == 'Entire rental unit', "Entire Unit")

# changing neighbourhood_cleansed and property_type as factors
df <- df %>%
  mutate(f_neighbourhood_cleansed = factor(neighbourhood_cleansed), f_property_type = factor(property_type))
df <- df %>% select(-neighbourhood_cleansed)
df <- df %>% select(-property_type)


#dropping `room_type` as there is only one value
unique(df$room_type)
df <- df %>% select(-room_type) #dropping `room_type` as there is only one value

# Add features: existing variables in different functional form
df <- df %>%
  mutate(price=as.numeric(price), price_daily=round(price*0.14), ln_price = log(price_daily), accommodates2 = accommodates^2,
         ln_review_scores_rating = log(review_scores_rating))


# Filtering for price since we had outliers like daily_price=62712, also the price was filtered to less than 350
summary(df$price_daily)

quantile(df$price_daily, .95)
df <- df %>% filter(price_daily<600)
# Squares and further values to create
df <- df %>%
  mutate(
    n_days_since2=n_days_since^2,
    n_days_since3=n_days_since^3, n_accommodates2=accommodates^2)

# Create dummy variables
dummies <- c("host_is_superhost", "host_identity_verified", "host_has_profile_pic" )
df <- df %>%
  mutate_at(vars(dummies), funs("d"= (.)))
dnames <- df %>%
  select(ends_with("_d")) %>%
  names()
dnames_i <- match(dnames, colnames(df))
colnames(df)[dnames_i] <- paste0("d_", tolower(gsub("[^[:alnum:]_]", "",dummies)))
df <- df %>% select(-c(host_is_superhost, host_has_profile_pic, host_identity_verified))

# Pool accommodations with 0,1,2,5 bathrooms
summary(df$bathrooms)
df <- df %>%
  mutate(f_bathroom = cut(bathrooms, c(0,1,2,5), labels=c(0,1,2), right = F))

# Pool num of reviews to 3 categories
summary(df$number_of_reviews)
df <- df %>%
  mutate(f_number_of_reviews = cut(number_of_reviews, c(0,1,51,max(df$number_of_reviews)), labels=c(0,1,2), right = F))

# Pool and categorize the number of minimum nights: 1,2,3, 3+
df <- df %>%
  mutate(f_minimum_nights= cut(minimum_nights, c(1,2,3,max(df$minimum_nights)), labels=c(1,2,3), right = F))



```



```{r include=FALSE, warning=FALSE, message=FALSE}
# Cleaning and creating dummies for the amenities variable
df$amenities <- gsub("\\[|\\]|\\\"|2013|2014|2019s|2019|\\\\u", "", df$amenities)
df$amenities <- as.list(strsplit(df$amenities, ","))
df$tv <- sapply(df$amenities, function(x) ifelse(length(grep("TV", x)) > 0, 1, 0))
df$heating <- sapply(df$amenities, function(x) ifelse(length(grep("Heating", x)) > 0, 1, 0))
df$wifi <- sapply(df$amenities, function(x) ifelse(length(grep("Wifi", x)) > 0, 1, 0))
df$coffee_maker <- sapply(df$amenities, function(x) ifelse(length(grep(" Coffee maker", x)) > 0, 1, 0))
df$AC <- sapply(df$amenities, function(x) ifelse(length(grep(" air conditioning", x)) > 0, 1, 0))
df$refrigerator <- sapply(df$amenities, function(x) ifelse(length(grep("Refrigerator", x)) > 0, 1, 0))
df$microwave <- sapply(df$amenities, function(x) ifelse(length(grep(" Microwave", x)) > 0, 1, 0))
df$hot_water <- sapply(df$amenities, function(x) ifelse(length(grep(" Hot water", x)) > 0, 1, 0))
df$long_term_stay <- sapply(df$amenities, function(x) ifelse(length(grep(" Long term stays allowed", x)) > 0, 1, 0))
df <- df %>% select(-amenities)
df <- select(df, -c(latitude, longitude))
df <- select(df, -(price))
df <- na.omit(df)
# save workfile
write.csv(df, paste0(data_out, "airbnb_cleaned.csv"))
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Looking for interactions.
  # It is a function it takes 3 arguments: 1) Your dataframe,
  # 2) the factor variable (like room_type)
  # 3)the dummy variable you are interested in (like TV)
price_diff_by_variables2 <- function(df, factor_var, dummy_var, factor_lab, dummy_lab){
  
  # Process your data frame and make a new dataframe which contains the stats
  factor_var <- as.name(factor_var)
  dummy_var <- as.name(dummy_var)
  
  stats <- df %>%
    group_by(!!factor_var, !!dummy_var) %>%
    dplyr::summarize(
      Mean = mean(price_daily, na.rm=TRUE),se = sd(price_daily)/sqrt(n()))
  
  stats[,2] <- lapply(stats[,2], factor)
  
  ggplot(
  stats, 
  aes_string(colnames(stats)[1], colnames(stats)[3], fill = colnames(stats)[2])) +
  geom_bar(
    stat='identity', 
    position = position_dodge(width=0.9), 
    alpha=0.8) +
  geom_errorbar(
    aes(ymin=Mean-(1.96*se),ymax=Mean+(1.96*se)),
    position=position_dodge(width = 0.9), 
    width = 0.25) +
  scale_color_manual(
    name=dummy_lab,
    values=c("#000000", "#990033", "#006699", "#33CC00", "#996633")) +
  scale_fill_manual(
    name=dummy_lab,
    values=c("#000000", "#990033", "#006699", "#33CC00", "#996633")) +
  ylab("Mean Price") +
  xlab(factor_lab) +
  theme_bw() +
  theme(
    panel.grid.major=element_blank(),
    panel.grid.minor=element_blank(),
    panel.border=element_blank(),
    axis.line=element_line(),
    legend.position = "top",
    legend.box = "vertical",
    legend.text = element_text(size = 5),
    legend.title = element_text(size = 5, face = "bold"),
    legend.key.size = unit(x = 0.4, units = "cm")
  )

}

```

```{r fig.height=4, message=FALSE, warning=FALSE, include=FALSE, out.width='50%'}
# Histograms
# price -> skewed distribution with long right tail
hist_price <- ggplot(data=df, aes(x=price_daily)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), boundary=0,
                 color = "white", fill = "#440154", alpha = 0.7) +
  coord_cartesian(xlim = c(0, 600)) +
  labs(x = "Price (US dollars)",y = "Percent", title = "Price Distribution For Copenhagen Apartments")+
  scale_y_continuous(labels = scales::percent_format(1)) +
  scale_x_continuous(limits=c(0,600)) + theme_bw() 


# lnprice -> closer to a normal distribution
hist_ln_price <- ggplot(data=df, aes(x=ln_price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.15,
                 color = "white", fill = "#440154", alpha = 0.7) +
  scale_y_continuous(labels = scales::percent_format(5L)) +
  labs(x = "ln(price, US dollars)",y = "Percent", " Log Price Distribution For Copenhagen Apartments")+
  theme_bw()
```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}
hist_price
hist_ln_price
```

Additionally, because the primary objective of this study is to develop price predictions for small and mid-size flats, the following categories for property types are filtered for the analysis: entire home or apartment, entire serviced apartment, entire condominium, entire loft, entire rental unit. According to the dictionary, there are various sorts of apartments, including lofts, condominiums and a rental unit referring to different types of apartments. The data set demonstrates the existence of a single room type. The figures below show the number of guests, and property type, as well as the mean prices for each which shows with increased number of accommodates price increases. While the property types apartment and lofts are higher priced than the two other categories. Also, room type variable have been dropped from the data It shows that there is only one room type across the entire data set.

```{r include=FALSE, message=FALSE, warning=FALSE}
# Boxplot for the number of accommodates
accomm <- ggplot(df,aes(factor(accommodates), price_daily, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Number of Accommodates", y = "Price") +
  ggtitle("Price by Number of Accommodates ")


# Boxplot for the number of beds
beds <- ggplot(df,aes(factor(beds), price_daily, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Number of beds", y = "Price") +
  ggtitle("Number of Beds Price Distribution")


# Boxplot for the Property type
property_type <- ggplot(df,aes(factor(f_property_type), price_daily, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Property Type", y = "Price") +
  ggtitle("Price by Property Type ")

```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}
accomm
property_type
```


```{r message=FALSE, warning=FALSE, include=FALSE}
################################################################################
# PART II - MODELLING
################################################################################


################################################################################
# Setting up models                 
################################################################################

# Basic Variables
basic_lev  <- c("accommodates", "beds", "f_property_type", "n_days_since", "flag_days_since", "f_bathroom")

# Factorized variables
basic_add <- c("f_neighbourhood_cleansed", "f_minimum_nights")
reviews <- c("review_scores_rating", "flag_review_scores_rating", "reviews_per_month", "f_number_of_reviews")
# Higher orders
poly_lev <- c("n_accommodates2", "n_days_since2", "n_days_since3")
# Dummy Variables 
dummies <- grep("^d_.*", names(df), value = TRUE)


################################################################################
# Interactions
################################################################################

#Look up property type interactions
p1 <- price_diff_by_variables2(df, "f_property_type", "AC", "Property Type", "Air Conditioning")
p2 <- price_diff_by_variables2(df, "f_property_type", "tv", "Property Type", "TV")
p3 <- price_diff_by_variables2(df, "f_property_type", "refrigerator", "Property Type", "Refrigerator")
p4 <- price_diff_by_variables2(df, "f_property_type", "wifi", "Property Type", "WIFI")
p5 <-price_diff_by_variables2(df, "f_property_type", "hot_water", "Property Type", "Hot Water")
p6 <- price_diff_by_variables2(df, "f_property_type", "microwave", "Property Type", "Microwave")
p7 <-  price_diff_by_variables2(df, "f_property_type", "heating", "Property Type", "Heating")
p8 <- price_diff_by_variables2(df, "f_property_type", "coffee_maker", "Property Type", "Coffee Maker")
p9 <- price_diff_by_variables2(df, "f_property_type", "accommodates", "Property Type", "Number of Accommodates")
g_interactions1 <- plot_grid(p1, p2, p3, p4, p5, p6, nrow=3, ncol=2) 
g_interactions2 <- plot_grid(p7, p8, p9, nrow=1, ncol=3) 
g_interactions1
g_interactions2


# based on suggested grpaphs
X1 <- c("f_property_type * accommodates")
# Additional dummies based on graphs suggestion
X2  <- c("f_property_type*AC", 
         "f_property_type*refrigerator",
         "f_property_type*tv",
         "f_property_type*wifi",
         "f_property_type*coffee_maker",
         "f_property_type*microwave", 
         "f_property_type*hot_water")

X3  <- c("f_property_type*f_neighbourhood_cleansed", "accommodates*f_neighbourhood_cleansed",
         paste0("(f_property_type) * (",
                paste(dummies, collapse=" + "),")"))

```

**New Variables** The second step in the process is to create new meaningful variables, such as the number of days since the first review, which is calculated by subtracting the date of the first review from the date the data was scraped and generating square, and cubic functional forms of the number of days since the first review and square form number of accommodates.

I have also grouped some numerical variables, such as the number of bathrooms. This variable contained data such as half bathroom. Another variable was created as a factor of bathrooms, with four cuts of 0, 1, 2, and 10. This means combining all of the variables in the aforementioned groups. Another example is the number of reviews for 0 to 51 and 51 and higher.

Other variables with missing values were addressed as follows: the first assumption is that each apartment has at least one bathroom; The missing number of beds was placed equal to number of accommodates, assuming the apartments having single beds. It is assumed that the minimum number of reviews is one. To indicate missing values in each predictor, flags were created.

As a result of data munging and preparation, the cleaned dataset has 9,244 observations and 54 variables.


# Data Analysis and Feature Engineering 

Feature engineering entails deciding on the type of predictor variables to include as well as the functional forms of predictors and potential interactions. The predictors are grouped as follows:

- **Basic variables** include the main predictors such as the number of accommodated, number of beds, property types, number of days since the first review, and its flag variable.

- **Basic addition** This includes key factorized variables like neighborhoods minimum nights. 

- **Review Variables** include important guest review predictors such as  review score rating and its flag indicating missing values, and factored total number of reviews.

- **Polynomial level** is made up of squared terms for number of accommodates as well as squared and cubic terms for days since the first review.

- **Dummies**, which included binary values for all amenities.

Finding the right interactions is the next step in the process. The plots below were used to visualize price changes across each interaction.

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}
g_interactions1
g_interactions2
```

Based on the plots above, I created three types of interactions: X1 which includes property type multiplied by the number of accommodates. X2 contains property type, air conditioning, refrigerator, WIFI, coffee maker, microwave and hot water dummy variables and X3 includes property types times neighborhood, accommodates times neighborhood groups, and all amenities.


```{r message=FALSE, warning=FALSE, include=FALSE}
# Create models in levels models: 1-8
modellev1 <- " ~ accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews, poly_lev),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1, X2),collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1, X2, dummies),collapse = " + "))
modellev8 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1, X2, dummies, X3),collapse = " + "))

model_table_view <- data.frame(Model = c('M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8'),
                               Predictors = c('Num of accommodates', 
                               'M1 + number of beds + property type + number of days since first review + room type', 
                               'M2 + Num bathrooms + Neighbourhood group + host reponse rate + reviews per month + 
                               reviews scores rating, flag review_scores rating", number_of_reviews',
                               'M3 + squared termof guests + squared and cubic terms of number of days since first review',
                               'M4 + property type and number of guests interaction + property type and room type interaction',
                               'M5 + property type interaction with dummies as air conditioning, TV, wifi, coffee_maker, microwave, hot_water',
                               'M6 + all other amenities',
                               'M7 + all other amenities, Neighbourhoods interacted with property type'))
model_table_view %>%
  kbl(caption = "Copenhagen Airbnb apartment price prediction Models", booktabs = T) %>%
  kable_classic(full_width = T, html_font = "Cambria")

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Separate hold-out set #
#----------------------------------------
# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(df))
# Set the random number generator: It will make results reproducable
set.seed(12022022)
# A) create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(df)), size = smp_size)
df$holdout <- 0
df$holdout[holdout_ids] <- 1


```



```{r message=FALSE, warning=FALSE, include=FALSE}
# Utilizing the Working data set:
#   a) estimating measures on the whole working sample (R2,BIC,RMSE)
#   b) Doing K-fold cross validation to get proper Test RMSE
## K = 5
k_folds=5
# Create the folds
set.seed(20230118)
#Hold-out set Set
data_holdout <- df %>% filter(holdout == 1)
#Working data set
data_work <- df %>% filter(holdout == 0)

folds_i <- sample(rep(1:k_folds, length.out = nrow(data_work)))
for (k in 1:k_folds) {
    test_i <- which(folds_i == k)}
    
data_train <- data_work[-test_i, ]

# Create results
model_names <- c()
model_nvars <- c()
model_bic <- c()
model_r2 <- c()
model_rmse_train <- c()
model_rmse_test <- c()


for (i in (1:8)){
  print(paste0( "Estimating model: " ,i ))
  
  # get model
  model_name <-  paste0("modellev",i)
  model_pretty_name <- paste0("(",i,")")

  # specify formula
  yvar <- "price_daily"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))

  # initialize values
  rmse_train <- c()
  rmse_test <- c()
  
  # estimate regression on the whole work data
  model_work_data <- lm(formula,data = data_work)
  BIC <- BIC(model_work_data)
  nvars <- model_work_data$rank -1
  r2 <- summary(model_work_data)$r.squared
  rmse_train = sqrt(sum(model_work_data$residuals^2)/nrow(data_work))
  
  # cross-validation
    cv_i = train(
    formula, data_work, method = 'lm', 
    trControl = trainControl(method = 'cv', number = k_folds)
  )
  
  rmse_test = sqrt(sum(cv_i$resample$RMSE^2)/k_folds)
  
  # gather key metrics
  model_names[i] <- model_pretty_name
  model_nvars[i] <- nvars
  model_bic[i] <- BIC
  model_r2[i] <- r2
  model_rmse_train[i] <- rmse_train
  model_rmse_test[i] <- rmse_test
}

# combine results
cv_result = data.frame(
  model_names, 
  model_nvars, 
  model_bic,
  model_r2, 
  model_rmse_train,
  model_rmse_test)
cv_result
  
colnames(cv_result) <- c('model', 'coefficients', "BIC", "R2", "RMSE_train", "RMSE_test")

colors = c("Training RMSE"="#000000","Test RMSE" = "#990033")


```

## Modeling
The best model provides the best prediction in the live data.  The original cleaned data is split into two random parts by 20% to 80% ratio in order to avoid over-fitting. The Holdout set includes 20% and the rest 80% is work data set. Then, the work dataset is split into train and test datasets and  5-fold cross validation is run on the train dataset. Then the best model is chosen based on the lowest average of 5 CV RMSE result. Eight basic OLS regression models from simplest to the most complex one were utilized to find the best model for our analysis. 5-fold cross-validation RMSE suggests that Model 7 regression has a better performance and it has the lowest RMSE test value of 59.2 dollars with 61 variables. The table is as following:

```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(cv_result, caption = "Evaluation of 8 Cross Validated Models", digits = 4, align = "lcccr", booktabs=T) %>% kable_styling(latex_options = c("HOLD_position", "resizebox=2.5\\textwidth"), font_size = 12, full_width = T) 
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Set lasso tuning parameters:
#----------------------------------------------------
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
# creating two predictors to be used for OLS, LASSO, Random forest, and GBM
predictors_1 <- c(basic_lev, basic_add, reviews, dummies)
predictors_2 <- c(basic_lev,basic_add,reviews,poly_lev, X1, X2, dummies) #Model 7, best model based on CV RMSE

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# OLS BASIC           
#################################
# Using OLS for the Basic variables
set.seed(12345)
system.time({
  ols_model <- train(
    formula(paste0("price_daily ~", paste0(predictors_2, collapse = " + "))),
    data = data_work,
    method = "lm",
    trControl = train_control
  )
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# LASSO               
# -------------------------------------------------
# setting seed
set.seed(120222)
system.time({
  lasso_model <- caret::train(
    formula(paste0("price_daily ~", paste0(predictors_2, collapse = " + "))),
    data = data_work,
    method = "glmnet",
    tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
    preProcess = c("center", "scale"),
    trControl = train_control
  )
})
print(lasso_model$bestTune$lambda)
lasso_coeffs <- coef(
  lasso_model$finalModel,
  lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>% 
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed
print(lasso_coeffs)
lasso_coeffs_nz <- lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])
regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_nz, by = "variable", all=TRUE)
regression_coeffs %>%
  write.csv(file = paste0(output, "regression_coeffs.csv"))

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# CART        
#-------------------------------------------
# setting seed
set.seed(12022022)
system.time({
  cart_model <- train(
    formula(paste0("price_daily ~", paste0(predictors_1, collapse = " + "))),
    data = data_work,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})
fancyRpartPlot(cart_model$finalModel, sub = "", palettes = "Purples")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# RANDOM FOREST     
#-------------------------------------------
# using all variables without their functional forms. Using predictor 1
# setting seed
# set tuning
tune_grid <- expand.grid(
  .mtry = c(8),
  .splitrule = "variance",
  .min.node.size = c(50)
)
# set seed
set.seed(12022022)
system.time({
rf_model <- train(
  formula(paste0("price_daily ~", paste0(predictors_1, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})

rf_model

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# GBM              
#----------------------
# Basic GMB model
gbm_grid <-  expand.grid(interaction.depth = c(5, 10), # complexity of the tree
                         n.trees = 250, # Number of trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)
set.seed(12345)
system.time({
  gbm_model <- train(formula(paste0("price_daily ~", paste0(predictors_1, collapse = " + "))),
                     data = data_work,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})

gbm_model

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Tuning parameter choice 1
result_1 <- matrix(c(
  rf_model$finalModel$mtry,
  rf_model$finalModel$min.node.size
),
nrow=1, ncol=2,
dimnames = list("Model A",
                c("Min vars","Min nodes")))
kable(x = result_1, format = "latex", digits = 3) %>%
  cat(.,file= paste0(output,"rf_models_turning_choices.tex"))
  
```

```{r message=FALSE, warning=FALSE, include=FALSE}
saveRDS(ols_model, paste0(data_out, 'OLS.rds'))
saveRDS(lasso_model, paste0(data_out, 'lasso.rds'))
saveRDS(cart_model, paste0(data_out, 'cart.rds'))
saveRDS(rf_model, paste0(data_out,'random_forest.rds'))
saveRDS(gbm_model, paste0(data_out,'gbm.rds'))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# FINAL MODELS         
#--------------------------
final_models <-
  list("OLS" = ols_model,
       "LASSO" = lasso_model,
       "CART" = cart_model,
       "Random forest"= rf_model,
       "GBM"  = gbm_model
       )
results <- resamples(final_models) %>% summary()
results
```

## Models

### OLS Model
The OLS model, which has a CR RMSE of 59.2742 an R-squared of 32.7% out performs GBM model with 105 predictors.

### LASSO
LASSO is an algorithm that fits a model by shrinking coefficients, some of which are reduced to zero by the addition of a penalty term. After performing 5-fold cross validation to determine the best value for lambda. Before running the models, the lasso tuning parameter is set, which acts as a weight for the penalty term versus the OLS fit. As a result, the strength of the variable selection is determined. The tuning parameter, Lamda, is set to a value 0.01 to 0.25 and the best tuned lambda with lowest RMSE is chosen as 0.01. LASSO outperforms all models in Holdout set with RMSE value of 57.6185 dollars and 5 fold cross validated average RMSE of 59.0655 dollars which is lower than GBM model.

### CART
CART also refereed as regression trees is an algorithm that has no formula; the goal is to produce a set of predictor bins. The bins are divided into smaller bins by this algorithm, and no functional forms or interactions are provided for CART.The CART model displays a 62.268 USD RMSE which is the highest among all models, thus, CART does not work well for our prediction.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
fancyRpartPlot(cart_model$finalModel, sub = "", palettes = "Purples") 
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Evaluating both data sets R squared
result_3r <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~Rsquared")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV Rsquared" = ".")
result_3r
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Evaluating both data sets
result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")
result_4
kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "") %>%
  cat(.,file= paste0(output,"final_models_cv_rmse.tex"))

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# evaluate preferred model on the holdout set -----------------------------
result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout), data_holdout[["price_daily"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")
result_5
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "") %>%
  cat(.,file= paste0(output,"final_models_houldout_rmse.tex"))
```
```{r message=FALSE, warning=FALSE, include=FALSE}
# Diagnsotics
#-------------------------------------------------
# Variable Importance Plots 
#-------------------------------------------------
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}
rf_model_var_imp <- importance(rf_model$finalModel)/1000
rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
```


### Random Forest
Random forest is an ensemble method in which the results of multiple predictive models are combined to generate a final prediction. For this model, I used basic variables, basic additions, reviews, and dummies with a minimum nod size of 50. The RMSE for the Holdout set is 58.497 dollars while the cross validated RMSE is the lowest among all models with value of 58.79 dollars.

### GBM
GBM is the final model for this case study. In terms of relative RMSE for 5-fold cross validation, it under performs OLS, LASSO, and Random Forest of 59.3699 dollars. The Rsqaured is also lower than OLS and LASSO with value of 32.54%.

**Result** The Random Forest model has a relatively better performance for 5-fold cross validated work set, while LASSO works best for holdout set, as can be seen from the table of models below. The data's 5-fold cross validation RMSE is 58.79 dollars for Random Forest which is the lowest among all models, which is 0.2735 dollars less than the RMSE for LASSO which is negligible. However, LASSO gives lowest RMSE for holdout set and the highest R-squared with value of 33.21% despite penalizing for interaction terms and shrinking some coefficients to zero. Considering external validity and model performance for live data, LASSO is chosen as the best model that has relative better performance in both sets.

```{r echo=FALSE, message=FALSE, warning=FALSE}
final_result4_5 <- cbind(result_4, result_5, result_3r)
final_result4_5 %>% 
  kbl(caption = "Evaluation of Models", digits = 4, align = "lcccr", booktabs=T) %>% 
  kable_minimal(full_width = F, html_font = "Cambria") %>% kable_styling(latex_options = c("HOLD_position", "resizebox=2.5\\textwidth"), font_size = 12, full_width = T) 
```


```{r include=FALSE, message=FALSE, warning=FALSE}
# full varimp plot, top 10 only
rf_model_var_imp_plot_b <- ggplot(
    rf_model_var_imp_df[1:10,], 
    aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='black', size=3) +
  geom_segment(
    aes(x=varname,xend=varname,y=0,yend=imp_percentage), 
    color='black', size=1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  labs(title = 'Simple variable importance plots') + 
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# 2) varimp plot grouped
#---------------------------------------------
# grouped variable importance - keep binaries created off factors together
varnames <- rf_model$finalModel$xNames
f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
groups <- list(f_neighbourhood_cleansed=f_neighbourhood_cleansed_varnames,
               f_bathroom = "f_bathroom",
               n_days_since = "n_days_since",
               accommodates = "n_accommodates",
               beds = "n_beds",
               reviews_per_month="reviews_per_month",
               review_scores_rating="review_scores_rating",
               f_minimum_nights="f_minimum_nights",
               f_number_of_reviews="f_number_of_reviews",
               f_property_type="f_property_type_varnames")
rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                            imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))
rf_model_var_imp_grouped_plot <- ggplot(
    rf_model_var_imp_grouped_df, 
    aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='black', size=3) +
  geom_segment(
    aes(x=varname,xend=varname,y=0,yend=imp_percentage), 
    color='black', size=1) +
  ylab("Importance (Percent)") +   
  xlab("Variable Name") +
  labs(title = 'Grouped variable importance plots') + 
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Partial Dependence Plots 
# ----------------------------------------------------------------
# Number of accommodates
pdp_n_acc <- pdp::partial(lasso_model, pred.var = "accommodates", pred.grid = distinct_(data_holdout, "accommodates"), train = data_train)
pdp_n_acc_plot <- pdp_n_acc %>%
  autoplot( ) +
  geom_point(color="#440154", size=3) +
  geom_line(color="#440154", size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") + ggtitle("Number of Accommodates Partial Dependence Plot") +
  scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
  theme_bw()

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Subsample performance: RMSE / mean(y) ---------------------------------------
data_holdout_w_prediction <- data_holdout %>%
  mutate(predicted_price = predict(lasso_model, newdata = data_holdout))
######### create nice summary table of heterogeneity
a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_daily),
    mean_price = mean(price_daily),
    rmse_norm = RMSE(predicted_price, price_daily) / mean(price_daily)
  )
b <- data_holdout_w_prediction %>%
  group_by(f_neighbourhood_cleansed) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_daily),
    mean_price = mean(price_daily),
    rmse_norm = rmse / mean_price
  )
c <- data_holdout_w_prediction %>%
  group_by(f_property_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_daily),
    mean_price = mean(price_daily),
    rmse_norm = rmse / mean_price
  )
d <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_daily),
    mean_price = mean(price_daily),
    rmse_norm = RMSE(predicted_price, price_daily) / mean(price_daily)
  )
# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")
line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("Neighbourhood", "", "", "")
result_3 <- rbind(line2, a, line1, c, line3, b, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))
result_3
options(knitr.kable.NA = '')
kable(x = result_3, format = "latex", booktabs=TRUE, linesep = "",digits = c(0,2,1,2), col.names = c("","RMSE","Mean price","RMSE/price")) %>%
  cat(.,file= paste0(output, "performance_across_subsamples.tex"))
options(knitr.kable.NA = NULL)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# FIGURES FOR FITTED VS ACTUAL OUTCOME VARIABLES #
##--------------------------------------------------
Ylev <- data_holdout[["price_daily"]]
# Predicted values
prediction_holdout_pred <- as.data.frame(predict(lasso_model, newdata = data_holdout, interval="predict")) 
predictionlev_holdout <- cbind(data_holdout[,c("price_daily","accommodates")],
                               prediction_holdout_pred)
# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,3] )
# Check the differences
d$elev <- d$ylev - d$predlev
# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = "#440154", size = 1,
             shape = 16, alpha = 0.5, show.legend=FALSE, na.rm=TRUE) +
  geom_segment(aes(x = 0, y = 0, xend = 350, yend =350), size=0.8, color="black", linetype=2) +
  labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)", title = "Actual vs Predicted Price") +
  theme_bw() 


```

## Diagnostics

LASSO is an algorithm that fits a model by shrinking the coefficients and even shrinks some of them to zero is chosen as the best model. Diagnostic tools, on the other hand, can be used to uncover information about the patterns of association that drive prediction. Some examples are as follows:

**Variable Importance Plot** depicts the average importance of fit when an x variable or set of x variables is used. Plot of variable importance for The top ten most important variables show that number of accommodates and beds, Indre By neighborhood, review variables are the most important. The importance of grouped variables reveals that neighborhoods, reviews, and days since first review are important.

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=3.5}
# ploting the variable importance plot
rf_model_var_imp_plot_b
rf_model_var_imp_grouped_plot
```

**Partial Dependence Plot** depicts how average y varies for different x values in relation to all other predictor variables. The partial dependence plot is based on the holdout set's predictors. The partial dependence plot for the number of accommodates and the price shows that the price rises as the number of accommodates increases.

**Performance Across Subsamples**
Examining the fit in different subsamples can provide information about the prediction's external validity.

**Actual vs Predicted Price** Another post-prediction diagnostic is a comparison of predicted and actual prices. The graph below shows that prediction performs better for lower prices than for higher prices. Howeve, for prices above 200 dollars, the model seems to not fit well.

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}
pdp_n_acc_plot
level_vs_pred
```

## Conclusion
The purpose of this report was to develop a more accurate model for predicting Airbnb prices in Copenhagen for small to mid-size apartments. Five models were depicted in order to compare and contrast their performance. The basic LASSO model with lambda value of 0.01 , which highlights meaningful characteristics about the nature of Airbnb apartments in Copenhagen was the best model with a 59.065 dollars RMSE, while it performed best in the holdout set with a 57.618 dollars RMSE. While OLS came in second place with 57.64 dollars RMSE for holdout set. In this study, simple models such as LASSO and OLS outperformed complicated models such as GBM and CART. Therefore, LASSO is chosen as the best model; however, results can be different depending on the dataset. The neighbouhood, days since first review, property type, number of bathrooms, the number of accommodations, review scores rating and reviews per month are key price drivers based on post prediction diagnostics. 


\newpage
## Appendix

```{r message=FALSE, echo=FALSE, warning=FALSE}
knitr::kable(model_table_view, caption = "Copenhagen Airbnb apartment price prediction Models", booktabs=T) %>% kable_styling(latex_options = c("HOLD_position", "resizebox=2.5\\textwidth"), font_size = 10, full_width = T) 
```

```{r echo=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
ggplot( data = cv_result, aes( x = factor( coefficients ) , group = 1 ) )+
  geom_line(aes( y = RMSE_train, color = 'Training RMSE'), size = 1 ) +
  geom_point(aes( y = RMSE_train, color = 'Training RMSE'), size = 2 ) + 
  geom_line(aes( y = RMSE_test , color = 'Test RMSE') , size = 1 ) +
  geom_point(aes( y = RMSE_test , color = 'Test RMSE') , size = 2 ) +
  labs(y='RMSE',x='Number of coefficients',color = "", title = "RMSE: Training & Test")+
  scale_color_manual(values = colors) + 
  scale_y_continuous(
    expand = expansion()) +
  theme_bw()+
  theme(legend.position=c(0.5, 0.8))
```
